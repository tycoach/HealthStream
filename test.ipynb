{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from minio import Minio\n",
    "from minio.error import S3Error\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "minio_endpoint = \"localhost:9000\"\n",
    "access_key =os.getenv('MINIO_ACCESS_KEY')\n",
    "secret_key = os.getenv('MINIO_SECRET')\n",
    "bucket_name = os.getenv('MINIO_BUCKET_NAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_minio_client(endpoint, access_key, secret_key, secure=False):\n",
    "#     return Minio(endpoint, access_key=access_key, secret_key=secret_key, secure=secure)\n",
    "def create_minio_client(endpoint, access_key, secret_key, secure=False):\n",
    "    client = Minio(endpoint, access_key=access_key, secret_key=secret_key, secure=secure)\n",
    "    print(f\"Created Minio client: {client}\")\n",
    "    return client\n",
    "\n",
    "def ensure_bucket(client, bucket_name):\n",
    "    if not client.bucket_exists(bucket_name):\n",
    "        client.make_bucket(bucket_name)\n",
    "        print(f\"Created bucket: {bucket_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client = Minio('localhost:9000', access_key=access_key, secret_key=secret_key, secure=False)\n",
    "# client\n",
    "\n",
    "\n",
    "\n",
    "def create_minio_client():\n",
    "    client = Minio(minio_endpoint, access_key=access_key, secret_key=secret_key, secure=False)\n",
    "    return client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minio.api.Minio at 0x7acee8bb6ff0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn = create_minio_client()\n",
    "conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MINIO_ENDPOINT = \"minio:9000\"\n",
    "\n",
    "\n",
    "#client = create_minio_client(MINIO_ENDPOINT,access_key, secret_key)\n",
    "bucket_name = \"rogerlake\"\n",
    "ensure_bucket(conn, bucket_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def run_spark_job():\n",
    "#     \"\"\"Dynamically fetch the latest file and run Spark transformation.\"\"\"\n",
    "#     client = create_minio_client()\n",
    "#     latest_raw_file = get_latest_raw_file(client)\n",
    "#     processed_parquet_file = get_processed_parquet_path()\n",
    "\n",
    "#     spark_command = [\n",
    "#         \"docker\", \"exec\", \"healthstream-spark-master\",  # Remove the \"-it\" flag\n",
    "#         \"/opt/bitnami/spark/bin/spark-submit\",\n",
    "#         \"--packages\", \"org.apache.hadoop:hadoop-aws:3.2.0\",\n",
    "#         \"--master\", \"spark://healthstream-spark-master:7077\",\n",
    "#         \"--executor-memory\", \"4g\",\n",
    "#         \"--driver-memory\", \"2g\",\n",
    "#         \"/opt/spark/jobs/transform_health_data.py\",\n",
    "#         f\"s3a://{bucket_name}/{latest_raw_file}\",\n",
    "#         f\"s3a://{bucket_name}/{processed_parquet_file}\"\n",
    "#     ]\n",
    "#     print(\"Running Spark job......\")\n",
    "    \n",
    "#     # For better debugging, capture output\n",
    "#     result = subprocess.run(\n",
    "#         spark_command,\n",
    "#         check=False,  # Change to False to avoid exceptions\n",
    "#         stdout=subprocess.PIPE,\n",
    "#         stderr=subprocess.PIPE,\n",
    "#         text=True\n",
    "#     )\n",
    "    \n",
    "#     # Print output for debugging\n",
    "#     print(f\"Return code: {result.returncode}\")\n",
    "#     print(f\"STDOUT: {result.stdout}\")\n",
    "#     print(f\"STDERR: {result.stderr}\")\n",
    "    \n",
    "#     # Now raise exception if needed\n",
    "#     if result.returncode != 0:\n",
    "#         raise Exception(f\"Spark job failed with return code {result.returncode}\")\n",
    "\n",
    "# # Task to run Spark job\n",
    "# spark_transform_task = PythonOperator(\n",
    "#     task_id=\"run_spark_transformation\",\n",
    "#     python_callable=run_spark_job,\n",
    "#     dag=dag,\n",
    "# )\n",
    "\n",
    "latest_raw_file = get_latest_raw_file(client)\n",
    "processed_parquet_file = get_processed_parquet_path()\n",
    "\n",
    "# spark_transform_task = SparkSubmitOperator(\n",
    "#     task_id='spark_transform',\n",
    "#     application='/opt/spark/jobs/transform_health_data.py',\n",
    "#     conn_id='spark_default',\n",
    "#     packages='org.apache.hadoop:hadoop-aws:3.2.0',\n",
    "#     env_vars={\n",
    "#         'JAVA_HOME': '/usr/lib/jvm/java-17-openjdk-amd64'\n",
    "#     },\n",
    "#     conf={\n",
    "#         'spark.master': 'spark://healthstream-spark-master:7077',\n",
    "#         'spark.executor.memory': '4g',\n",
    "#         'spark.driver.memory': '2g'\n",
    "#     },\n",
    "#     application_args=[\n",
    "#         f\"s3a://{bucket_name}/{latest_raw_file}\",\n",
    "#         f\"s3a://{bucket_name}/{processed_parquet_file}\"\n",
    "#     ],\n",
    "#     dag=dag\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FROM apache/airflow:2.10.5\n",
    "\n",
    "# Switch to root for initial setup\n",
    "USER root\n",
    "\n",
    "# Install system dependencies\n",
    "\n",
    "RUN apt-get update && \\\n",
    "    apt-get install -y --no-install-recommends \\\n",
    "    build-essential \\\n",
    "    default-jdk \\\n",
    "    git \\\n",
    "    curl \\\n",
    "    unzip \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Set Java environment variables\n",
    "ENV JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64\n",
    "ENV PATH=$JAVA_HOME/bin:$PATH\n",
    "\n",
    "\n",
    "# Copy requirements\n",
    "COPY ../requirements.txt /requirements.txt\n",
    "\n",
    "# Ensure correct file ownership\n",
    "RUN chown airflow:root /requirements.txt\n",
    "\n",
    "# Switch to airflow user\n",
    "USER airflow\n",
    "\n",
    "# Install dependencies\n",
    "RUN pip install --upgrade pip && \\\n",
    "    pip install -r /requirements.txt\n",
    "\n",
    "# # We'll install faker directly since we can't access generator folder at build time\n",
    "RUN pip install faker\n",
    "RUN pip install tqdm\n",
    "RUN pip install minio\n",
    "RUN pip install apache-airflow-providers-apache-spark\n",
    "\n",
    "# Set PYTHONPATH\n",
    "#ENV PYTHONPATH=\"/opt/airflow:/opt/airflow/dags:/opt/airflow/generator\"\n",
    "ENV PYTHONPATH=\"/opt/airflow:/opt/airflow/dags:/opt/airflow/generator\"\n",
    "\n",
    "\n",
    "WORKDIR /opt/airflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from airflow import DAG\n",
    "from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator\n",
    "from airflow.operators.python import PythonOperator\n",
    "import os\n",
    "from datetime import datetime\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from minio import Minio\n",
    "import subprocess\n",
    "from dotenv import load_dotenv\n",
    "from airflow.models import Variable\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "bucket_name = \"rogerlake\"\n",
    "# minio_endpoint = os.getenv(\"MINIO_ENDPOINT\", \"172.18.0.2:9000\")\n",
    "minio_endpoint = Variable.get(\"MINIO_ENDPOINT\")\n",
    "access_key = Variable.get(\"MINIO_ACCESS_KEY\")\n",
    "print(f\"Using access\", access_key)\n",
    "secret_key = Variable.get(\"MINIO_SECRET\")\n",
    "print(f\"------Connecting to MinIO at: {minio_endpoint}\")\n",
    "print(f\"Using bucket: {bucket_name}\")\n",
    "\n",
    "\n",
    "\n",
    "# Ensure Python can find the 'generator' module\n",
    "sys.path.append('/opt/airflow')\n",
    "sys.path.append('/opt/airflow/generator')\n",
    "\n",
    "from generator.minio_client import create_minio_client\n",
    "# Define environment variables\n",
    "RAW_PREFIX = \"health_records/_raw_data_/health_record_raw_\"\n",
    "PROCESSED_PREFIX = \"health_records/_cleaned_/health_data_\"\n",
    "\n",
    "client = create_minio_client()\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "parquet_object = \"health_records/_cleaned_/health_data_{timestamp}.parquet\"\n",
    "\n",
    "\n",
    "\n",
    "def get_latest_raw_file(minio_client):\n",
    "    \"\"\"Retrieve the latest raw data file from MinIO.\"\"\"\n",
    "    raw_files = list(minio_client.list_objects(bucket_name, prefix=RAW_PREFIX))\n",
    "    if not raw_files:\n",
    "        raise FileNotFoundError(\" No raw files found in MinIO!\")\n",
    "    # Extract timestamps from filenames\n",
    "    latest_file = max(raw_files, key=lambda x: x.last_modified)\n",
    "    return latest_file.object_name\n",
    "\n",
    "# Function to generate processed file path with timestamp\n",
    "def get_processed_parquet_path():\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    return f\"{PROCESSED_PREFIX}{timestamp}.parquet\"\n",
    "\n",
    "# latest_raw_file = get_latest_raw_file(client)\n",
    "# processed_parquet_file = get_processed_parquet_path()\n",
    "\n",
    "# Default args for the DAG\n",
    "default_args = {\n",
    "    \"owner\": \"airflow\",\n",
    "    \"depends_on_past\": False,\n",
    "    \"start_date\": datetime(2024, 1, 1),\n",
    "    \"email_on_failure\": False,\n",
    "    \"retries\": 1,\n",
    "    \"retry_delay\": timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "# Initialize DAG\n",
    "dag = DAG(\n",
    "    \"health_data_etl\",\n",
    "    default_args=default_args,\n",
    "    description=\"ETL pipeline for processing health records with Spark\",\n",
    "    schedule_interval=\"@daily\",\n",
    "    catchup=False,\n",
    "    tags=[\"spark\", \"minio\", \"etl\"],\n",
    ")\n",
    "\n",
    "# Fetch Latest Raw File\n",
    "fetch_latest_raw_file_task = PythonOperator(\n",
    "    task_id=\"fetch_latest_raw_file\",\n",
    "    python_callable=get_latest_raw_file(client),\n",
    "    do_xcom_push=True,  # Save result in XCom for later use\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "# Generate Processed File Path\n",
    "generate_parquet_path_task = PythonOperator(\n",
    "    task_id=\"generate_parquet_path\",\n",
    "    python_callable=get_processed_parquet_path,\n",
    "    do_xcom_push=True,  # Save result in XCom for later use\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "#  Run Spark Job\n",
    "\n",
    "spark_transform_task = SparkSubmitOperator(\n",
    "    task_id='spark_transform',\n",
    "    application='/opt/spark/jobs/transform_health_data.py',\n",
    "    conn_id='spark_default',  # Ensure this is correctly configured in Airflow Connections\n",
    "    packages='org.apache.hadoop:hadoop-aws:3.2.0',\n",
    "    env_vars={\n",
    "        'JAVA_HOME': '/usr/lib/jvm/java-17-openjdk-amd64',\n",
    "        'SPARK_HOME': '/opt/spark',\n",
    "        'PATH':  '/opt/spark/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', #\n",
    "        'PYTHONPATH': '/opt/spark/jobs:/opt/spark/python:/opt/spark/python/lib/py4j-0.10.9.5-src.zip:$PYTHONPATH'\n",
    "        \n",
    "    },\n",
    "    conf={\n",
    "        'spark.master': 'spark://healthstream-spark-master:7077',\n",
    "        'spark.executor.memory': '4g',\n",
    "        'spark.driver.memory': '2g',\n",
    "        \"spark.submit.deployMode\": \"client\",\n",
    "        'spark.executor.extraJavaOptions': '-Dfile.encoding=UTF-8',\n",
    "        'spark.driver.extraJavaOptions': '-Dfile.encoding=UTF-8',\n",
    "        'spark.hadoop.fs.s3a.path.style.access': 'true',\n",
    "        'spark.hadoop.fs.s3a.connection.ssl.enabled': 'false',\n",
    "        'spark.files': '/opt/spark/jobs/transform_health_data.py'\n",
    "        #'spark.jars': '/opt/bitnami/spark/jars/hadoop-aws-3.3.2.jar,/opt/bitnami/spark/jars/aws-java-sdk-bundle-1.11.1026.jar'\n",
    "    },\n",
    "    application_args=[\n",
    "        \"{{ ti.xcom_pull(task_ids='fetch_latest_raw_file') }}\",\n",
    "        \"{{ ti.xcom_pull(task_ids='generate_parquet_path') }}\"\n",
    "    ],\n",
    "    # application_args=[\n",
    "    #     f\"s3a://{bucket_name}/{latest_raw_file}\",\n",
    "    #     f\"s3a://{bucket_name}/{processed_parquet_file}\"\n",
    "    # ],\n",
    "    verbose=True,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "\n",
    "#  Verify Output\n",
    "def verify_output():\n",
    "    \"\"\"Check if Parquet file exists in MinIO.\"\"\"\n",
    "    if client.stat_object(bucket_name, parquet_object):\n",
    "        print(f\" Processed data available: {parquet_object}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\" Parquet file not found!\")\n",
    "\n",
    "verify_output_task = PythonOperator(\n",
    "    task_id=\"verify_parquet_output\",\n",
    "    python_callable=verify_output,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Task Dependencies\n",
    "fetch_latest_raw_file_task >> generate_parquet_path_task >> spark_transform_task >> verify_output_task\n",
    "# spark_transform_task >> verify_output_task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main function to orchestrate the ETL process.\"\"\"\n",
    "    parser = argparse.ArgumentParser(description='Transform health data from CSV to Parquet')\n",
    "    parser.add_argument('input_path', help='Input S3 path (can be auto-detected if not provided)',\n",
    "                        nargs='?', default=None)\n",
    "    parser.add_argument('output_path', help='Output S3 path for Parquet files',\n",
    "                        nargs='?', default=None)\n",
    "    parser.add_argument('--bucket', help='MinIO/S3 bucket name', \n",
    "                        default='healthdata')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    \n",
    "    # Initialize clients\n",
    "    spark = initialize_spark()\n",
    "    client = create_minio_client()\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    \n",
    "    try:\n",
    "        # Determine input path\n",
    "        if args.input_path:\n",
    "            input_path = args.input_path\n",
    "        else:\n",
    "            # Auto-detect latest file\n",
    "            latest_object = get_latest_file(client, bucket_name, \"_raw_data_\")\n",
    "            input_path = f\"s3a://{bucket_name}/{latest_object}\"\n",
    "        \n",
    "        # Determine output path\n",
    "        if args.output_path:\n",
    "            output_path = args.output_path\n",
    "        else:\n",
    "            output_path = f\"s3a://{bucket_name}/cleaned_data/health_record_{timestamp}.parquet\"\n",
    "        \n",
    "        print(f\"Processing: {input_path} -> {output_path}\")\n",
    "        \n",
    "        # Clean and transform data\n",
    "        cleaned_df = clean_transform_data(spark, input_path)\n",
    "        \n",
    "        # Save to parquet\n",
    "        save_to_parquet(cleaned_df, output_path)\n",
    "        \n",
    "        print(\"Data transformation completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in ETL process: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        # Clean up\n",
    "        spark.stop()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
